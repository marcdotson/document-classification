{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# All of the follwoing is code that is typically used with NLP and Text Analysis"
      ],
      "metadata": {
        "id": "8iSf6bm_VkzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Text Cleaning / NLP Pipeline\n"
      ],
      "metadata": {
        "id": "eld1RcPfV4Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Importing all required libraries for text cleaning.\n",
        "Includes libraries for text processing, web scraping, tokenization, and more.\n",
        "'''\n",
        "\n",
        "import re  # For regular expressions\n",
        "import string  # For string operations\n",
        "import nltk  # For natural language processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup  # For web scraping (if needed)\n",
        "import contractions  # For expanding contractions (e.g., can't -> cannot)\n",
        "import spacy  # For advanced NLP tasks\n",
        "from nltk.tokenize.toktok import ToktokTokenizer  # Toktok tokenizer for tokenization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Spacy model download command (if not already installed)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load Spacy language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initialize Toktok tokenizer\n",
        "tokenizer = ToktokTokenizer()\n"
      ],
      "metadata": {
        "id": "_brF9x1BZMLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to convert all text to lowercase for consistency.\n",
        "'''\n",
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "text = \"Natural Language Processing is Fun!\"\n",
        "lowercased_text = lowercase_text(text)\n",
        "print(lowercased_text)\n"
      ],
      "metadata": {
        "id": "nwAXAz8iZinN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to remove punctuation from the text.\n",
        "'''\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "text = \"Hello! How are you? NLP is great.\"\n",
        "cleaned_text = remove_punctuation(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "0bPQKlwxZkiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Another way to remove specific items using the re library.\n",
        "Useful to locate regualar expressions. USE chatgpt to help with this. Ask for how to remove regualr expresssions in python using re library and the syntax\n",
        "'''\n",
        "stripped_text = soup.get_text()\n",
        "stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "stripped_text = re.sub(r\"[^'\\w\\s\\.]\", '', stripped_text)\n",
        "stripped_text = re.sub(r'\\d+', '', stripped_text)"
      ],
      "metadata": {
        "id": "hR0gCHFjZ__T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to remove any numbers from the text.\n",
        "d stands for digits the + means one or more\n",
        "'''\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "text = \"I have 2 cats and 1 dog.\"\n",
        "no_numbers_text = remove_numbers(text)\n",
        "print(no_numbers_text)"
      ],
      "metadata": {
        "id": "0D2XiAgOZypV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to remove extra spaces from the text.\n",
        "'''\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "text = \"This  is   an example   with  extra spaces.\"\n",
        "cleaned_text = remove_extra_spaces(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "1lIzx7N1sYb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to expand contractions (e.g., can't -> cannot).\n",
        "'''\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "text = \"I can't believe it's happening.\"\n",
        "expanded_text = expand_contractions(text)\n",
        "print(expanded_text)\n"
      ],
      "metadata": {
        "id": "wEnpgAsMtCNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to remove stop words from the text using the Toktok tokenizer and NLTK stopwords list.\n",
        "'''\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "text = \"This is an example sentence showing stopword removal.\"\n",
        "cleaned_text = remove_stopwords(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "IC9uK_6ttIiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to tokenize text into words using NLTK's word tokenizer.\n",
        "'''\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "text = \"Natural language processing with Python.\"\n",
        "tokens = tokenize_text(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Qib5HCcgtIXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to lemmatize the text using Spacy.\n",
        "Lemmatization converts words to their base form (e.g., \"running\" -> \"run\").\n",
        "'''\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text\n",
        "\n",
        "text = \"The striped bats are hanging on their feet for best.\"\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "id": "_EaTijpKtIAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " -Function that combines all the processes above"
      ],
      "metadata": {
        "id": "zdyFck0Etrjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function that combines all the text cleaning steps into one.\n",
        "Includes lowercasing, punctuation removal, number removal, extra space removal, contraction expansion, stop word removal, and lemmatization.\n",
        "'''\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = lowercase_text(text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = remove_punctuation(text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = remove_numbers(text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = remove_extra_spaces(text)\n",
        "\n",
        "    # Expand contractions\n",
        "    text = expand_contractions(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "    # Lemmatize text\n",
        "    text = lemmatize_text(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "text = \"Natural Language Processing can't be ignored in 2024! It's crucial.\"\n",
        "cleaned_text = clean_text(text)\n",
        "print(cleaned_text)\n"
      ],
      "metadata": {
        "id": "bR0ScvkBtlz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Basic Text Statistics"
      ],
      "metadata": {
        "id": "obWy2TDwtxm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **word_tokenize()**: Breaks a string into individual words (tokens).\n",
        "* **sent_tokenize()**: Splits a string into individual sentences.\n",
        "* **Counter()**: Creates a frequency distribution for tokens or characters.\n",
        "* **most_common()**: Returns the most frequent items in a list or dictionary.\n",
        "* **set()**: Stores unique elements, automatically removing duplicates.\n",
        "* **re.sub()**: Substitutes all occurrences of a regex pattern with another string."
      ],
      "metadata": {
        "id": "F960Y6GZvQAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Libraries"
      ],
      "metadata": {
        "id": "wqGpNBENv8Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Importing necessary libraries for text statistics.\n",
        "- nltk: Used for tokenization and linguistic processing.\n",
        "- re: Regular expressions for pattern matching in text.\n",
        "- collections: Provides useful data structures like Counter to calculate word frequencies.\n",
        "- pandas: Used for organizing data into DataFrames (tabular format).\n",
        "'''\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "YmaYM4E1tlug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word counter\n"
      ],
      "metadata": {
        "id": "Qd0Shftjv5mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to count the total number of words in a text.\n",
        "- Tokenizing: We use nltk's word_tokenize to break the text into individual words.\n",
        "- len(): Counts the number of tokens (words) in the list.\n",
        "\n",
        "Input: A string of text.\n",
        "Output: The total number of words.\n",
        "'''\n",
        "def word_count(text):\n",
        "    tokens = word_tokenize(text)  # Tokenizing text into words\n",
        "    return len(tokens)  # Returning the total number of tokens (words)\n",
        "\n",
        "text = \"Natural language processing is fun. Let's count the words!\"\n",
        "total_words = word_count(text)\n",
        "print(\"Total word count:\", total_words)\n"
      ],
      "metadata": {
        "id": "O-PUD1Y3tllg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sentance counter"
      ],
      "metadata": {
        "id": "RzxL4hR1wACS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to count the number of sentences in a text.\n",
        "- sent_tokenize: A function from nltk that splits the text into sentences.\n",
        "- len(): Counts the number of sentences in the list.\n",
        "\n",
        "Input: A string of text.\n",
        "Output: The total number of sentences.\n",
        "'''\n",
        "def sentence_count(text):\n",
        "    sentences = sent_tokenize(text)  # Tokenizing text into sentences\n",
        "    return len(sentences)  # Returning the total number of sentences\n",
        "\n",
        "total_sentences = sentence_count(text)\n",
        "print(\"Total sentence count:\", total_sentences)\n"
      ],
      "metadata": {
        "id": "y9zOJj8fwBWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average word length"
      ],
      "metadata": {
        "id": "SQ58lVAbwUoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " '''\n",
        "Function to calculate the average length of words in the text.\n",
        "- Tokenizing text into words.\n",
        "- Using list comprehension to calculate the length of each word.\n",
        "- sum(): Adds all word lengths together.\n",
        "- len(): Divides the total sum by the number of words to get the average.\n",
        "\n",
        "Input: A string of text.\n",
        "Output: Average word length (float).\n",
        "'''\n",
        "def average_word_length(text):\n",
        "    tokens = word_tokenize(text)  # Tokenizing text into words\n",
        "    word_lengths = [len(word) for word in tokens]  # Calculating the length of each word\n",
        "    return sum(word_lengths) / len(word_lengths)  # Calculating the average word length\n",
        "\n",
        "avg_word_length = average_word_length(text)\n",
        "print(\"Average word length:\", avg_word_length)\n"
      ],
      "metadata": {
        "id": "1Ar22bfAwUMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word Frequency (Top N Words)"
      ],
      "metadata": {
        "id": "zed9k5zpwlg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to calculate word frequency and return the top N most common words.\n",
        "- Counter(): Creates a dictionary where the keys are the words, and the values are the word counts.\n",
        "- most_common(): A method in Counter that returns the N most common words.\n",
        "\n",
        "Input: A string of text and the number N of top words to return.\n",
        "Output: List of tuples containing the top N words and their counts.\n",
        "'''\n",
        "def word_frequency(text, N=5):\n",
        "    tokens = word_tokenize(text)  # Tokenizing text into words\n",
        "    frequency = Counter(tokens)  # Calculating the frequency of each word\n",
        "    return frequency.most_common(N)  # Returning the top N most frequent words\n",
        "\n",
        "top_words = word_frequency(text, 3)\n",
        "print(\"Top 3 words:\", top_words)\n"
      ],
      "metadata": {
        "id": "jdDURlV5wlAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}