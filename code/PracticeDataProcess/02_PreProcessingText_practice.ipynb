{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ksbuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ksbuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Importing all required libraries for text cleaning.\n",
    "Includes libraries for text processing, web scraping, tokenization, and more.\n",
    "'''\n",
    "\n",
    "import re  # For regular expressions\n",
    "import string  # For string operations\n",
    "import nltk  # For natural language processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup  # For web scraping (if needed)\n",
    "import contractions  # For expanding contractions (e.g., can't -> cannot)\n",
    "import spacy  # For advanced NLP tasks\n",
    "from nltk.tokenize.toktok import ToktokTokenizer  # Toktok tokenizer for tokenization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Spacy model download command (if not already installed)\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load Spacy language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize Toktok tokenizer\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each excel sheet into individuals data frames\n",
    "\n",
    "df1 = pd.read_excel(r\"C:\\Users\\ksbuf\\OneDrive\\Desktop\\Invista PRoject\\document-classification\\data\\Roomba Reviews.xlsx\", sheet_name = 'iRobot Roomba 650')\n",
    "df2 = pd.read_excel(r\"C:\\Users\\ksbuf\\OneDrive\\Desktop\\Invista PRoject\\document-classification\\data\\Roomba Reviews.xlsx\", sheet_name = 'iRobot Roomba 880')\n",
    "\n",
    "#combined dataframes into one\n",
    "df_combined = pd.concat([df1,df2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_combined.drop(['Date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Product'] = df_cleaned['Product'].replace({'iRobot Roomba 650 for Pets': '650', 'iRobot Roomba 880 for Pets and Allergies': '880'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truly a wonderful thing.Reminded me of that old Peter, Paul & Mary song, Marvelous Toy.\"  Truly a wonderful thing.\n"
     ]
    }
   ],
   "source": [
    "#Look for any null values in our reviews and see if they can be filled in with context from the title \n",
    "for title in df_cleaned['Title'][df_cleaned['Review'].isna()]:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it appears that this could be a review itself rather than a title, the title appears to be \"Truly a wonderful thing.\" So we can make assumptions and fix this\n",
    "\n",
    "df_cleaned[df_cleaned['Review'].isna()].head()\n",
    "\n",
    "#split up the title and the review\n",
    "df_cleaned.loc[240, 'Title'] = 'Truly a wonderful thing.'\n",
    "df_cleaned.loc[240, 'Review'] = 'Reminded me of that old Peter, Paul & Mary song, Marvelous Toy.\" Truly a wonderful thing.'\n",
    "\n",
    "#check to make sure the values are correct\n",
    "df_cleaned.loc[240].head()\n",
    "\n",
    "df_cleaned['Review'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksbuf\\AppData\\Local\\Temp\\ipykernel_30400\\4000539047.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_cleaned['Received Five Stars'] = df_cleaned['Received Five Stars'].replace({'Five Stars': 1, 'Not Five Stars': 0})\n"
     ]
    }
   ],
   "source": [
    "df_cleaned['Received Five Stars'] = df_cleaned['Rating']\n",
    "df_cleaned = df_cleaned.drop('Rating', axis = 1)\n",
    "df_cleaned['Received Five Stars'] = df_cleaned['Received Five Stars'].replace({'Five Stars': 1, 'Not Five Stars': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lower() and strip() to both 'Title' and 'Review' columns\n",
    "df_cleaned[['Title', 'Review']] = df_cleaned[['Title', 'Review']].apply(lambda x: x.str.lower().str.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up our contractions prior to us going through and removing the punctuation \n",
    "df_cleaned['Title'] = df_cleaned['Title'].fillna('').apply(contractions.fix)\n",
    "\n",
    "df_cleaned['Review'] = df_cleaned['Review'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all of our punctionation\n",
    "\n",
    "df_cleaned['Title'] = df_cleaned['Title'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)\n",
    "df_cleaned['Review'] = df_cleaned['Review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove extra spaces from the 'Title' and 'Review' columns\n",
    "df_cleaned[['Title', 'Review']] = df_cleaned[['Title', 'Review']].apply(lambda x: x.str.split().str.join(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any stop words from our text columns\n",
    "\n",
    "# Tokenize the text in each row of the column, remove stopwords, and join the tokens back\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply the stopword removal to the 'Review' column without a function\n",
    "df_cleaned['Review'] = df_cleaned['Review'].apply(\n",
    "    lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "df_cleaned['Review'] = df_cleaned['Review'].apply(lemmatize_text)\n",
    "df_cleaned['Title'] = df_cleaned['Title'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('roomba', 2291)\n",
      "('clean', 2200)\n",
      "('get', 1774)\n",
      "('vacuum', 1282)\n",
      "('floor', 1249)\n",
      "('time', 1115)\n",
      "('work', 985)\n",
      "('one', 970)\n",
      "('well', 934)\n",
      "('room', 930)\n",
      "('hair', 911)\n",
      "('would', 893)\n",
      "('go', 826)\n",
      "('use', 825)\n",
      "('run', 803)\n",
      "('day', 771)\n",
      "('house', 769)\n",
      "('every', 683)\n",
      "('thing', 673)\n",
      "('like', 665)\n"
     ]
    }
   ],
   "source": [
    "#check for top 20 most common words and see if we need to create a unique stop words list to drop these words\n",
    "def word_frequency(text, N):\n",
    "    tokens = word_tokenize(text)  # Tokenizing text into words\n",
    "    frequency = Counter(tokens)  # Calculating the frequency of each word\n",
    "    return frequency.most_common(N)  # Returning the top N most frequent words\n",
    "\n",
    "text = ' '.join(df_cleaned['Review'].astype(str))\n",
    "top_words = word_frequency(text, 20)\n",
    "\n",
    "for word in top_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clean', 2200)\n",
      "('vacuum', 1282)\n",
      "('floor', 1249)\n",
      "('time', 1115)\n",
      "('work', 985)\n",
      "('one', 970)\n",
      "('well', 934)\n",
      "('room', 930)\n",
      "('hair', 911)\n",
      "('would', 893)\n",
      "('use', 825)\n",
      "('run', 803)\n",
      "('day', 771)\n",
      "('house', 769)\n",
      "('every', 683)\n",
      "('pick', 643)\n",
      "('much', 640)\n",
      "('good', 639)\n",
      "('dog', 628)\n",
      "('great', 591)\n"
     ]
    }
   ],
   "source": [
    "# create a list with the additional stop words that we should remove or would not be super useful\n",
    "\n",
    "more_stop_words = [ 'roomba', 'get', 'go', 'thing', 'like']\n",
    "\n",
    "df_cleaned['Review'] = df_cleaned['Review'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in more_stop_words]))\n",
    "#check for top 20 most common words and see if we need to create a unique stop words list to drop these words\n",
    "def word_frequency(text, N):\n",
    "    tokens = word_tokenize(text)  # Tokenizing text into words\n",
    "    frequency = Counter(tokens)  # Calculating the frequency of each word\n",
    "    return frequency.most_common(N)  # Returning the top N most frequent words\n",
    "\n",
    "text = ' '.join(df_cleaned['Review'].astype(str))\n",
    "top_words = word_frequency(text, 20)\n",
    "\n",
    "for word in top_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will be of more use for us if we are able to take all of our text and combine it into a text column\n",
    "\n",
    "df_cleaned['All text'] = df_cleaned['Title'] + ' ' + df_cleaned['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 5), (922, 5))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sp;it out our data that will be used to train the model and the data that we will then test the model on\n",
    "'''\n",
    "\n",
    "# Creating DataFrame for test data where 'Received Five Stars' is NaN\n",
    "df_test_data = df_cleaned[df_cleaned['Received Five Stars'].isna()]\n",
    "\n",
    "# Creating DataFrame for training data where 'Received Five Stars' is not NaN\n",
    "df_training_data = df_cleaned[df_cleaned['Received Five Stars'].notna()]\n",
    "\n",
    "# Getting the shapes of both DataFrames\n",
    "test_data_shape = df_test_data.shape\n",
    "training_data_shape = df_training_data.shape\n",
    "\n",
    "test_data_shape, training_data_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_excel(r\"C:\\Users\\ksbuf\\OneDrive\\Desktop\\Invista PRoject\\document-classification\\data\\cleaned_data_roomba.xlsx\", index=False)\n",
    "df_test_data.to_excel(r\"C:\\Users\\ksbuf\\OneDrive\\Desktop\\Invista PRoject\\document-classification\\data\\test_data_roomba.xlsx\", index=False)\n",
    "df_training_data.to_excel(r\"C:\\Users\\ksbuf\\OneDrive\\Desktop\\Invista PRoject\\document-classification\\data\\training_data_roomba.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
